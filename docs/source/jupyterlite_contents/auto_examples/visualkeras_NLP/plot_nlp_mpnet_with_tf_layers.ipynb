{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the scikit-plots examples in JupyterLite is experimental and you may encounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import scikitplot`can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/scikit-plots/scikit-plots/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# visualkeras transformers example\n\nAn example showing the :py:func:`~scikitplot.visualkeras` function\nused by a :py:class:`~tensorflow.keras.Model` or :py:class:`~torch.nn.Module` or :py:class:`~transformers.TFPreTrainedModel` model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: The scikit-plots developers\n# SPDX-License-Identifier: BSD-3-Clause\n\n# Force garbage collection\nimport gc; gc.collect()\nimport tensorflow as tf\n# Clear any session to reset the state of TensorFlow/Keras\ntf.keras.backend.clear_session()\n\nfrom transformers import TFAutoModel\n\nfrom scikitplot import visualkeras\n\n# Load the Hugging Face transformer model\ntransformer_model = TFAutoModel.from_pretrained(\"microsoft/mpnet-base\")\n\n# Define a Keras-compatible wrapper for the Hugging Face model\ndef wrap_transformer_model(inputs):\n    input_ids, attention_mask = inputs\n    outputs = transformer_model(input_ids=input_ids, attention_mask=attention_mask)\n    return outputs.last_hidden_state  # Return the last hidden state for visualization\n\n# Define Keras model inputs\ninput_ids = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\nattention_mask = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n\n# Pass inputs through the transformer model using a Lambda layer\nlast_hidden_state = tf.keras.layers.Lambda(\n    wrap_transformer_model,\n    output_shape=(128, 768),  # Explicitly specify the output shape\n    name='microsoft_mpnet-base',\n)([input_ids, attention_mask])\n\n# Reshape the output to fit into Conv2D (adding extra channel dimension) inside a Lambda layer\n# def reshape_last_hidden_state(x):\n#     return tf.reshape(x, (-1, 1, 128, 768))\n# reshaped_output = tf.keras.layers.Lambda(reshape_last_hidden_state)(last_hidden_state)\n# Use Reshape layer to reshape the output to fit into Conv2D (adding extra channel dimension)\n# Reshape to (batch_size, 128, 768, 1) for Conv2D input\nreshaped_output = tf.keras.layers.Reshape((-1, 128, 768))(last_hidden_state)\n\n# Add different layers to the model\nx = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='conv2d_1')(reshaped_output)\nx = tf.keras.layers.BatchNormalization(name='batchnorm_1')(x)\nx = tf.keras.layers.Dropout(0.3, name='dropout_1')(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name='maxpool_1')(x)\n\nx = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv2d_2')(x)\nx = tf.keras.layers.BatchNormalization(name='batchnorm_2')(x)\nx = tf.keras.layers.Dropout(0.3, name='dropout_2')(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name='maxpool_2')(x)\n\nx = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2d_3')(x)\nx = tf.keras.layers.BatchNormalization(name='batchnorm_3')(x)\nx = tf.keras.layers.Dropout(0.4, name='dropout_3')(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name='maxpool_3')(x)\n\n# Add GlobalAveragePooling2D before the Dense layers\nx = tf.keras.layers.GlobalAveragePooling2D(name='globalaveragepool')(x)\n\n# Add Dense layers\nx = tf.keras.layers.Dense(512, activation='relu', name='dense_1')(x)\nx = tf.keras.layers.Dropout(0.5, name='dropout_4')(x)\nx = tf.keras.layers.Dense(128, activation='relu', name='dense_2')(x)\n\n# Add output layer (classification head)\ndummy_output = tf.keras.layers.Dense(2, activation='softmax', name=\"dummy_classification_head\")(x)\n\n# Wrap into a Keras model\nwrapped_model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=dummy_output)\n\n# Visualize the wrapped model\nimg_nlp_mpnet_with_tf_layers = visualkeras.layered_view(\n    wrapped_model,\n    legend=True,\n    show_dimension=True,\n    scale_xy=1, scale_z=1, max_z=250,\n    to_file=\"../result_images/nlp_mpnet_with_tf_layers.png\"\n)\ntry:\n    import matplotlib.pyplot as plt\n    plt.imshow(img_nlp_mpnet_with_tf_layers)\n    plt.axis('off')\n    plt.show()\nexcept:\n    pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
